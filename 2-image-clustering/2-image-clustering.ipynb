{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5a50ff-a078-4832-b05a-47a05315ce9a",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Multimodal Workshop\n",
    "## Image clustering with Multimodal Embeddings\n",
    "\n",
    "In this Jupyter Notebook, we will explore the power of multimodal embeddings to cluster images into different groups. Our goal is to classify images into three categories: kitchen, bedroom, and bathroom. We will leverage the capabilities of vector databases and Amazon Titan Multimodal Embeddings in Amazon Bedrock to achieve this task. Additionally we will make use of Amazon Bedrock Batch Inference to get the embeddings for a larger number of images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33745d62-6d47-457d-a361-bdbd7fd1d125",
   "metadata": {},
   "source": [
    "#### Preview SDK for Batch Inference\n",
    "At the time of creating this notebook, Batch Inference for Amazon Bedrock is still in public preview. To complete this notebook we will download and install the boto3 and botocore clients versions which include the preview.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d536ab-f0a4-4aba-b095-1e34a66c5689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import process_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3949f-1b52-4976-b8e2-11b1c578a1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_zip(\"https://d2eo22ngex1n9g.cloudfront.net/Documentation/SDK/bedrock-python-sdk-reinvent.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296c22e-e5dd-4bf9-a295-47f63b04f65e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall botocore boto3 -qy  \n",
    "!pip install -q preview-sdk/botocore-1.32.4-py3-none-any.whl\n",
    "!pip install -q preview-sdk/boto3-1.29.4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09920510-906f-4b20-ab6d-15f2e726afdc",
   "metadata": {},
   "source": [
    "### Install and import needed libraries\n",
    "For this notebook to run correctly, we will need to install, import and initialize the necessary libraries and clients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5219323-0d72-40d6-bf7c-104340fe3f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f27e32-d21b-4b6f-9c38-6d5b73608e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from utils import resize_image, process_zip\n",
    "from pinecone import Pinecone, PodSpec\n",
    "\n",
    "# Boto3 clients\n",
    "s3_client = boto3.client('s3')\n",
    "iam_client = boto3.client('iam')\n",
    "sts_client = boto3.client('sts')\n",
    "bedrock_client = boto3.client('bedrock')\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "# Account and region info\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb2594-d6f9-489d-a7b1-53f1f8ff47e0",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Titan Multimodal Embeddings (TMME)\n",
    "In this section of the notebook we will create the functions needed to retrieve embeddings using Amazon Bedrock TMME. \n",
    "\n",
    "#### Define output embedding length\n",
    "Titan Multimodal Embeddings gives you the option to create embeddings with three vector sizes: 1024, 384 or 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b93bd-e5e5-4505-94d3-b4dcfb020494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputEmbeddingLength = 1024 # Define output vector size – 1,024 (default), 384, 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916d8d9-32a4-45ef-a230-55f95e735b0c",
   "metadata": {},
   "source": [
    "#### Image embeddings\n",
    "This function will transform an image into an embeddings vector using TMME. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bbe7b-13f9-42cd-a64d-49b129d53fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings_of_image(image, outputEmbeddingLength = outputEmbeddingLength):\n",
    "    with open(image, \"rb\") as image_file:\n",
    "        imageEncoded = base64.b64encode(image_file.read()).decode('utf8')\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"inputImage\": imageEncoded,\n",
    "            \"embeddingConfig\": { \n",
    "                \"outputEmbeddingLength\": outputEmbeddingLength\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=\"amazon.titan-embed-image-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    vector = json.loads(response['body'].read().decode('utf8'))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d43918-d96a-417e-8d55-c8797bf3e32f",
   "metadata": {},
   "source": [
    "### Pinecone Vector Database\n",
    "\n",
    "[Pinecone](https://www.pinecone.io) is a vector database that allows you to store and retrieve high-dimensional vectors efficiently. In this notebook, we will be using Pinecone to store the image embeddings generated from our image data.\n",
    "\n",
    "Vector databases like Pinecone are particularly useful for similarity search tasks, where you want to find the closest matches to a given query vector. By storing our image embeddings in Pinecone, we can easily perform tasks like image similarity search, clustering, and recommendation systems.\n",
    "\n",
    "Before we can start using Pinecone, we need to set up a Pinecone account and create an index. An index in Pinecone is a collection of vectors that can be queried and updated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a7b5d-d263-4b5f-8f28-998dd9529ae2",
   "metadata": {},
   "source": [
    "### Store embeddings in the vector database\n",
    "Now we know how to get embeddings for our content, we are going to store them in a vector database to later on query and retrieve results. \n",
    "\n",
    "To complete this section you will need to create a free account with Pinecone which includes a free index to test and retrieve your API key. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a55f0-7961-4951-b550-478a94e08c45",
   "metadata": {},
   "source": [
    "#### Create the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b00195-59fd-457a-a835-65be5f5ef566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinecone =  Pinecone(api_key=\"#INSERT YOUR API KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae000cca-4956-4f1f-9cc7-a4e434144558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_name = \"house-rooms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73aac8-8d07-4dff-93c4-b456475cd72a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinecone.create_index(index_name,\n",
    "        dimension=outputEmbeddingLength,\n",
    "        metric='cosine',\n",
    "        spec=PodSpec(environment=\"gcp-starter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f790b-2828-42d4-8553-34e0eeedb66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinecone.describe_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5757ab-d2e9-4b7a-beb3-0056284b0a12",
   "metadata": {},
   "source": [
    "### Store images into the vector database -- Index 3 images (clustering groups)\n",
    "\n",
    "After creating the Pinecone index, the next step is to generate embeddings for your images and then upload (ingest) those embeddings into the index. The process will involve resizing the images to the maximum height and width supported by the Amazon Titan Multimodal Embeddings model. Additionally, it will create associated metadata for each vector, which includes the type of room the image represents.\n",
    "\n",
    "The process can be broken down into the following steps:\n",
    "\n",
    "1. **Image Resizing**: Before generating embeddings, you'll need to resize your images to the maximum dimensions supported by the Amazon Titan Multimodal Embeddings model. This step ensures that the model can process the images correctly and generate accurate embeddings.\n",
    "\n",
    "2. **Embedding Generation**: After resizing the images, you'll use the Amazon Titan Multimodal Embeddings model to generate embeddings for each image. These embeddings are high-dimensional vector representations that capture the visual features and semantics of the images.\n",
    "\n",
    "3. **Metadata Creation**: Along with the embeddings, you'll create associated metadata for each image. This metadata will include information about the type of room the image represents, such as \"living room,\" \"bedroom,\" or \"kitchen\" and the image path to later retrieve. \n",
    "\n",
    "4. **Data Preparation**: Before ingesting the embeddings and metadata into the Pinecone index, you'll need to prepare your data. This involves creating a list or generator that yields tuples of (vector_embeddings, image_ids, metadata) for each image.\n",
    "\n",
    "5. **Ingestion into Pinecone Index**: Finally, you'll use the `index.upsert()` method from the Pinecone Python client to ingest the embeddings and associated metadata into the index. This method takes a list or generator of (vector_embeddings, image_path, room_type) tuples as input. You can specify the batch size for ingestion to optimize performance.\n",
    "\n",
    "After completing this process, your Pinecone index will be populated with the image embeddings and their associated metadata, including the room types. This will enable you to query the index for similar images based on their embeddings and retrieve the closest vector, retrieving the room type from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2b4f5-5266-44a7-94db-ea5d5834baca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a90e6-68e7-4a96-9da9-ff1b54b21664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_info_to_vectordb(vector, image, image_name, type, index):\n",
    "    index.upsert([\n",
    "        (image_name, vector[\"embedding\"], {\"path\": image, \"type\": type})\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea145f5-49da-49e5-b307-b0ee65112fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_folder = \"base-images\"\n",
    "for image_name in os.listdir(images_folder):\n",
    "    if image_name.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(images_folder, image_name)\n",
    "        imagename_without_extension = os.path.splitext(image_name)[0]\n",
    "        type = imagename_without_extension.split(\"_\")[0]\n",
    "        image = Image.open(image_path)\n",
    "        if (image.size[0] > 2048 or image.size[1] > 2048):\n",
    "            resize_image(image_path)\n",
    "        print(\"Indexing:\", image_path)\n",
    "        vector = get_embeddings_of_image(image_path)\n",
    "        send_info_to_vectordb(vector, image_path, imagename_without_extension, type, index)\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb158f-3940-4e93-9d11-7c82aaea3d27",
   "metadata": {},
   "source": [
    "### Compare a sample image to retrieve room type\n",
    "\n",
    "This section of the notebook demonstrates how to use multimodal embeddings and Pinecone classifiying the images into different room types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42023449-f332-436f-91dd-1a3ae77f00d0",
   "metadata": {},
   "source": [
    "#### Image query\n",
    "With this function we will first transform our image query into an embeddings vector, which we will then use to query the vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20034bc-2d75-47fd-b790-f9ee7cc6f8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_the_database_with_image(image):\n",
    "    vector = get_embeddings_of_image(image)[\"embedding\"]\n",
    "    results = index.query(\n",
    "        vector=vector,\n",
    "        top_k=1,\n",
    "        include_metadata=True,\n",
    "        include_values=True\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b862a5-50e4-4411-9df4-82067ba60722",
   "metadata": {},
   "source": [
    "#### Cluster the test images\n",
    "With this function we will first cluster our test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea4f86-a944-45e1-99e5-033920c8d10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_images_classification():\n",
    "    images_folder = \"test-images\"\n",
    "    for image_name in os.listdir(images_folder):\n",
    "       if image_name.endswith(\".jpg\"):\n",
    "           image_path = os.path.join(images_folder, image_name)\n",
    "           image = Image.open(image_path)\n",
    "           if (image.height > 2048 or image.width > 2048):\n",
    "                resize_image(image_path)\n",
    "           results = query_the_database_with_image(image_path)\n",
    "           print(\"Photo: {} is type {}\".format(image_path, results[\"matches\"][0][\"metadata\"][\"type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a9ab4-0559-4c71-bd97-46c5a730f021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_images_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ee862-c764-4817-aeb5-2a2fb78f918d",
   "metadata": {},
   "source": [
    "### Create embeddings at large scale with Amazon Bedrock Batch Inference\n",
    "\n",
    "With batch inference, you can run multiple inference requests asynchronously to process a large number of requests efficiently by running inference on data that is stored in an S3 bucket. You can use batch inference to improve the performance of model inference on large datasets. In this section you will explore how to prepare the dataset and run an Amazon Bedrock batch inference job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8fc45-bc1e-48da-a1c7-2d61238f66fa",
   "metadata": {},
   "source": [
    "#### Create an Amazon S3 bucket\n",
    "Create an bucket where your input/output data will be stored.\n",
    "\n",
    "If you already have a bucket created, replace the name in the next cell and skip the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3c2ad-ded3-464a-aa54-634689d20037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name = \"amazonbr-batch-embeddings-{}-{}\".format(account_id, region)\n",
    "s3_bucket_path = \"s3://{}\".format(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcaef2-c00e-4181-acaa-370874f9c244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if region != 'us-east-1':\n",
    "        s3_client.create_bucket(\n",
    "            Bucket=bucket_name,     \n",
    "            CreateBucketConfiguration={\n",
    "                'LocationConstraint': region\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "    print(\"AWS Bucket: {}\".format(bucket_name))\n",
    "except Exception as err:\n",
    "    print(\"ERROR: {}\".format(err))\n",
    "\n",
    "s3_bucket_path = \"s3://{}\".format(bucket_name)\n",
    "print(\"S3 bucket path: {}\".format(s3_bucket_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96d9b2-811c-49fd-addf-e0026be061ed",
   "metadata": {},
   "source": [
    "### Batch inference preparation - Creating role and policies requirements\n",
    "\n",
    "We will now prepare the necessary role for the batch inference job. That includes creating the policies required to run model invocation jobs with Amazon Bedrock.\n",
    "\n",
    "#### Create Trust relationship\n",
    "This JSON object defines the trust relationship that allows the bedrock service to assume a role that will give it the ability to talk to other required AWS services. The conditions set restrict the assumption of the role to a specfic account ID and a specific component of the bedrock service (model_invocation_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b58caf-9544-4f3f-a425-6235bcbee8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role_name = \"AmazonBedrockModelInvocation-batch-embeddings-3\"\n",
    "s3_bedrock_ft_access_policy=\"AmazonBedrock-batch-embeddings-3\"\n",
    "embeddings_model_arn= \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacdb4ff-9d04-46af-b13f-3ebb3cc240e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROLE_DOC = f\"\"\"{{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {{\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {{\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            }},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {{\n",
    "                \"StringEquals\": {{\n",
    "                    \"aws:SourceAccount\": \"{account_id}\"\n",
    "                }},\n",
    "                \"ArnEquals\": {{\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:{region}:{account_id}:model-invocation-job/*\"\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5804c-5c1f-4571-8d17-2835f4b6a57f",
   "metadata": {},
   "source": [
    "### Create S3 access policy\n",
    "\n",
    "This JSON object defines the permissions of the role we want bedrock to assume to allow access to the S3 bucket that we created that will hold our prompts and allow certain bucket and object manipulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc2b82-d4cd-447d-96ab-bb2755de6bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACCESS_POLICY_DOC = f\"\"\"{{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {{\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:GetBucketAcl\",\n",
    "                \"s3:GetBucketNotification\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutBucketNotification\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{bucket_name}\",\n",
    "                \"arn:aws:s3:::{bucket_name}/*\"\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc581e-6b38-4c74-a5f6-fd1d5b9c2e88",
   "metadata": {},
   "source": [
    "### Create IAM role and attach policies\n",
    "\n",
    "Let's now create the IAM role with the created trust policy and attach the s3 policy to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d17dc9-1a30-4fe3-a81d-13300a49fc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = iam_client.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=ROLE_DOC,\n",
    "    Description=\"Role for Bedrock to access S3 for model invocation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a54f5-d9b9-4c81-973f-e95df69c0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = response[\"Role\"][\"Arn\"]\n",
    "response = iam_client.create_policy(\n",
    "    PolicyName=s3_bedrock_ft_access_policy,\n",
    "    PolicyDocument=ACCESS_POLICY_DOC,\n",
    ")\n",
    "policy_arn = response[\"Policy\"][\"Arn\"]\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=policy_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c62964-9978-44df-9e90-8a6444666f9a",
   "metadata": {},
   "source": [
    "### Configure the model invocation job\n",
    "#### Create the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518f29c-3740-4fdb-a2b6-3d564983c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"batch-images\"\n",
    "input_key = \"input.jsonl\"\n",
    "output_path = \"validation/output/\"\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "data = []\n",
    "\n",
    "# Supported image file extensions\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "\n",
    "# Iterate over files in the folder\n",
    "for i, file_name in enumerate(os.listdir(folder_path)):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    _, extension = os.path.splitext(file_path)\n",
    "    if extension.lower() in image_extensions:\n",
    "        image = Image.open(file_path)\n",
    "        if image.height > 2048 or image.width > 2048:\n",
    "            resize_image(file_path)\n",
    "\n",
    "        model_input = {\n",
    "            \"inputImage\": image_to_base64(file_path),\n",
    "            \"embeddingConfig\": {\n",
    "                \"outputEmbeddingLength\": outputEmbeddingLength \n",
    "            }\n",
    "        }\n",
    "\n",
    "        data.append({'recordId': file_name, 'modelInput': model_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3f375-0a34-4417-b533-eb4d63dba9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input data items are:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096387e-b06e-483c-b18c-e452eb77402f",
   "metadata": {},
   "source": [
    "#### Process data and output to new lines\n",
    "The model invocation job requires the input data to be in jsonl format and located Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882dc51-7952-4e02-97b6-ad071b6e40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"\"\n",
    "for row in data:\n",
    "    output_data += json.dumps(row) + \"\\n\"\n",
    "\n",
    "s3_client.put_object(Body=output_data, Bucket=bucket_name, Key=input_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d690fd7-b8a0-4879-bef2-7fd8ab68189d",
   "metadata": {},
   "source": [
    "#### Define data configuration and launch job\n",
    "As the input data is prepared and uploaded to Amazon S3 we can go ahead and launch the invocation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10e362-9949-4ec8-80cb-65ddfe8618cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3InputFormat\": \"JSONL\",\n",
    "        \"s3Uri\": \"{}/{}\".format(s3_bucket_path, input_key)\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": \"{}/{}\".format(s3_bucket_path, output_path)\n",
    "    }\n",
    "})\n",
    "date_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "response = bedrock_client.create_model_invocation_job(\n",
    "    roleArn=role_arn,\n",
    "    modelId=embeddings_model_arn,\n",
    "    jobName=f\"my-batch-job-test-{date_time}\",\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")\n",
    "\n",
    "jobArn = response.get('jobArn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aca610-386a-4a10-ac81-239753d5b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "status = bedrock_client.get_model_invocation_job(jobIdentifier=jobArn)['status']\n",
    "while status not in [\"Completed\", \"Failed\", \"Stopping\", \"Stopped\"]:\n",
    "    status = bedrock_client.get_model_invocation_job(jobIdentifier=jobArn)['status']\n",
    "    print(status)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a81d4-3997-4940-b93a-dc8dcb5946cb",
   "metadata": {},
   "source": [
    "#### Retrieve the embeddings \n",
    "Once the batch job is complete we can go ahead and download the output file and extract the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34480ee1-75cd-4659-847e-4e0dba9c959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = jobArn.split(\"/\")[-1]\n",
    "images_file = \"input.jsonl.out\"\n",
    "s3_client.download_file(bucket_name, \"{}{}/{}\".format(output_path, job_id, images_file), images_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664317a-bcff-4454-bddb-17f1fef5ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_embedding_list = []\n",
    "with open('input.jsonl.out', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        record_id = data['recordId']\n",
    "        embedding = data['modelOutput']['embedding']\n",
    "        record_embedding = {'recordId': record_id, 'embedding': embedding}\n",
    "        record_embedding_list.append(record_embedding)\n",
    "print(\"The embedding list contains {} records.\".format(len(record_embedding_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d07b6-fa57-496d-a588-7c118acbee5c",
   "metadata": {},
   "source": [
    "#### Classify the embeddings\n",
    "Now we have the embeddings from our batch job we can query them against the vector database to retrieve the room type of each picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59b0cb-e1f9-4611-bfb7-bd29c4657e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_the_database_with_vector(vector):\n",
    "    results = index.query(\n",
    "        vector=vector,\n",
    "        top_k=1,\n",
    "        include_metadata=True,\n",
    "        include_values=True\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_batch_job_classification(record_embedding_list):\n",
    "       for record in record_embedding_list:\n",
    "        vector = record['embedding']\n",
    "        image_id = record['recordId']\n",
    "\n",
    "        results = query_the_database_with_vector(vector)\n",
    "        photo_type = results[\"matches\"][0][\"metadata\"][\"type\"]\n",
    "        print(f\"Photo: {image_id} is type {photo_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e0b74-64a8-4bda-9d16-5757be098689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_batch_job_classification(record_embedding_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b43883-0d3e-4a2d-ae9b-bdddcb755279",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "In this section we will delete any resource which may incur in unnecessary costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9df69-33e0-47cc-96ed-6efea7ef52ed",
   "metadata": {},
   "source": [
    "#### Delete the Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733dce5c-10e0-4ee7-a036-b61d434b4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all objects in the bucket\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            s3_client.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "        print(f\"All objects in {bucket_name} have been deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting objects from {bucket_name}: {e}\")\n",
    "\n",
    "# Delete the bucket\n",
    "try:\n",
    "    response = s3_client.delete_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket {bucket_name} has been deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting bucket {bucket_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9e3c6-da65-4b0d-a55f-766b7a6f14cf",
   "metadata": {},
   "source": [
    "#### Delete the Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385de4d-7c3d-47bd-abf6-bad13446d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
