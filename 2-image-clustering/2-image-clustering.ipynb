{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5a50ff-a078-4832-b05a-47a05315ce9a",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Multimodal Workshop\n",
    "## Image clustering with Multimodal Embeddings\n",
    "\n",
    "In this Jupyter Notebook, we will explore the power of multimodal embeddings to cluster images into different groups. Our goal is to classify images into three categories: kitchen, bedroom, and bathroom. We will leverage the capabilities of vector databases and Amazon Titan Multimodal Embeddings in Amazon Bedrock to achieve this task. Additionally we will make use of Amazon Bedrock Batch Inference to get the embeddings for a larger number of images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09920510-906f-4b20-ab6d-15f2e726afdc",
   "metadata": {},
   "source": [
    "### Install and import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5219323-0d72-40d6-bf7c-104340fe3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f27e32-d21b-4b6f-9c38-6d5b73608e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from utils import resize_image, process_zip\n",
    "from pinecone import Pinecone, PodSpec\n",
    "\n",
    "# Boto3 clients\n",
    "s3_client = boto3.client('s3')\n",
    "iam_client = boto3.client('iam')\n",
    "sts_client = boto3.client('sts')\n",
    "bedrock_client = boto3.client('bedrock')\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "# Account and region info\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33745d62-6d47-457d-a361-bdbd7fd1d125",
   "metadata": {},
   "source": [
    "#### Preview SDK for Batch Inference\n",
    "At the time of creating this notebook, Batch Inference for Amazon Bedrock is still in public preview. To complete this notebook we will download and install the boto3 and botocore clients versions which include the preview.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3949f-1b52-4976-b8e2-11b1c578a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_zip(\"https://d2eo22ngex1n9g.cloudfront.net/Documentation/SDK/bedrock-python-sdk-reinvent.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296c22e-e5dd-4bf9-a295-47f63b04f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall botocore boto3 -qy  \n",
    "!pip install -q preview-sdk/botocore-1.32.4-py3-none-any.whl\n",
    "!pip install -q preview-sdk/boto3-1.29.4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb2594-d6f9-489d-a7b1-53f1f8ff47e0",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Titan Multimodal Embeddings (TMME)\n",
    "In this section of the notebook we will create the functions needed to retrieve embeddings using Amazon Bedrock TMME. \n",
    "\n",
    "#### Define output embedding length\n",
    "Titan Multimodal Embeddings gives you the option to create embeddings with three vector sizes: 1024, 384 or 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b93bd-e5e5-4505-94d3-b4dcfb020494",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputEmbeddingLength = 1024 # Define output vector size – 1,024 (default), 384, 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916d8d9-32a4-45ef-a230-55f95e735b0c",
   "metadata": {},
   "source": [
    "#### Image embeddings\n",
    "This function will transform an image into an embeddings vector using TMME. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bbe7b-13f9-42cd-a64d-49b129d53fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_of_image(image, outputEmbeddingLength = outputEmbeddingLength):\n",
    "    with open(image, \"rb\") as image_file:\n",
    "        imageEncoded = base64.b64encode(image_file.read()).decode('utf8')\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"inputImage\": imageEncoded,\n",
    "            \"embeddingConfig\": { \n",
    "                \"outputEmbeddingLength\": outputEmbeddingLength\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=\"amazon.titan-embed-image-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    vector = json.loads(response['body'].read().decode('utf8'))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8e8de-27e9-4b52-a747-9bb089bd46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download and prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d43918-d96a-417e-8d55-c8797bf3e32f",
   "metadata": {},
   "source": [
    "### Pinecone Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a7b5d-d263-4b5f-8f28-998dd9529ae2",
   "metadata": {},
   "source": [
    "### Store embeddings in a vector database\n",
    "Now we know how to get embeddings for our content, we are going to store them in a vector database to later on query and retrieve results. \n",
    "\n",
    "In this case we are going to use Pinecone.\n",
    "\n",
    "To complete this section you will need to create a free account with Pinecone which includes a free index to test and retrieve your API key. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a55f0-7961-4951-b550-478a94e08c45",
   "metadata": {},
   "source": [
    "#### Create the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b00195-59fd-457a-a835-65be5f5ef566",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone =  Pinecone(api_key=\"Replace with Your Pinecone API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae000cca-4956-4f1f-9cc7-a4e434144558",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"house-rooms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73aac8-8d07-4dff-93c4-b456475cd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.create_index(index_name,\n",
    "        dimension=outputEmbeddingLength,\n",
    "        metric='cosine',\n",
    "        spec=PodSpec(environment=\"gcp-starter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f790b-2828-42d4-8553-34e0eeedb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.describe_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5757ab-d2e9-4b7a-beb3-0056284b0a12",
   "metadata": {},
   "source": [
    "### Store images into the vector database -- Index 3 images (clustering groups)\n",
    "Now our Pinecone index is created, we can start ingesting our images embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2b4f5-5266-44a7-94db-ea5d5834baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a90e6-68e7-4a96-9da9-ff1b54b21664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_info_to_vectordb(vector, image, image_name, type, index):\n",
    "    index.upsert([\n",
    "        (image_name, vector[\"embedding\"], {\"path\": image, \"type\": type})\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea145f5-49da-49e5-b307-b0ee65112fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = \"base-images\"\n",
    "for image_name in os.listdir(images_folder):\n",
    "    if image_name.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(images_folder, image_name)\n",
    "        imagename_without_extension = os.path.splitext(image_name)[0]\n",
    "        type = imagename_without_extension.split(\"_\")[0]\n",
    "        image = Image.open(image_path)\n",
    "        if (image.size[0] > 2048 or image.size[1] > 2048):\n",
    "            resize_image(image_path)\n",
    "        print(\"Indexing:\", image_path)\n",
    "        vector = get_embeddings_of_image(image_path)\n",
    "        send_info_to_vectordb(vector, image_path, imagename_without_extension, type, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cb158f-3940-4e93-9d11-7c82aaea3d27",
   "metadata": {},
   "source": [
    "### Compare a sample image to retrieve classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42023449-f332-436f-91dd-1a3ae77f00d0",
   "metadata": {},
   "source": [
    "#### Image query\n",
    "With this function we will first transform our image query into an embeddings vector, which we will then use to query the vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20034bc-2d75-47fd-b790-f9ee7cc6f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_the_database_with_image(image):\n",
    "    vector = get_embeddings_of_image(image)[\"embedding\"]\n",
    "    results = index.query(\n",
    "        vector=vector,\n",
    "        top_k=1,\n",
    "        include_metadata=True,\n",
    "        include_values=True\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b862a5-50e4-4411-9df4-82067ba60722",
   "metadata": {},
   "source": [
    "#### Cluster the test images\n",
    "With this function we will first cluster some test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea4f86-a944-45e1-99e5-033920c8d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_images_classification():\n",
    "    images_folder = \"test-images\"\n",
    "    for image_name in os.listdir(images_folder):\n",
    "       if image_name.endswith(\".jpg\"):\n",
    "           image_path = os.path.join(images_folder, image_name)\n",
    "           image = Image.open(image_path)\n",
    "           if (image.height > 2048 or image.width > 2048):\n",
    "                resize_image(image_path)\n",
    "           results = query_the_database_with_image(image_path)\n",
    "           #print(results)\n",
    "           print(\"Photo: {} is type {}\".format(image_path, results[\"matches\"][0][\"metadata\"][\"type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a9ab4-0559-4c71-bd97-46c5a730f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ee862-c764-4817-aeb5-2a2fb78f918d",
   "metadata": {},
   "source": [
    "### Create embeddings at large scale with Amazon Bedrock Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8fc45-bc1e-48da-a1c7-2d61238f66fa",
   "metadata": {},
   "source": [
    "#### Create an Amazon S3 bucket\n",
    "Create an bucket where your input/output data will be stored.\n",
    "\n",
    "If you already have a bucket created, replace the name in the next cell and skip the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3c2ad-ded3-464a-aa54-634689d20037",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"amazonbr-batch-embeddings-{}-{}\".format(account_id, region)\n",
    "s3_bucket_path = \"s3://{}\".format(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcaef2-c00e-4181-acaa-370874f9c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if region != 'us-east-1':\n",
    "        s3_client.create_bucket(\n",
    "            Bucket=bucket_name,     \n",
    "            CreateBucketConfiguration={\n",
    "                'LocationConstraint': region\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "    print(\"AWS Bucket: {}\".format(bucket_name))\n",
    "except Exception as err:\n",
    "    print(\"ERROR: {}\".format(err))\n",
    "\n",
    "s3_bucket_path = \"s3://{}\".format(bucket_name)\n",
    "print(\"S3 bucket path: {}\".format(s3_bucket_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96d9b2-811c-49fd-addf-e0026be061ed",
   "metadata": {},
   "source": [
    "### Batch inference preparation - Creating role and policies requirements\n",
    "\n",
    "We will now prepare the necessary role for the batch inference job. That includes creating the policies required to run model invocation jobs with Amazon Bedrock.\n",
    "\n",
    "#### Create Trust relationship\n",
    "This JSON object defines the trust relationship that allows the bedrock service to assume a role that will give it the ability to talk to other required AWS services. The conditions set restrict the assumption of the role to a specfic account ID and a specific component of the bedrock service (model_invocation_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b58caf-9544-4f3f-a425-6235bcbee8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = \"AmazonBedrockModelInvocation-batch-embeddings\"\n",
    "s3_bedrock_ft_access_policy=\"AmazonBedrock-batch-embeddings\"\n",
    "embeddings_model_arn= \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1\"\n",
    "embeddings_model_arn_2 = \"arn:aws:bedrock:us-east-1:947565228676:custom-model/amazon.titan-embed-image-v1:0/wrmjvnf7a922\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacdb4ff-9d04-46af-b13f-3ebb3cc240e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_DOC = f\"\"\"{{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {{\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {{\n",
    "                \"Service\": \"bedrock.amazonaws.com\"\n",
    "            }},\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Condition\": {{\n",
    "                \"StringEquals\": {{\n",
    "                    \"aws:SourceAccount\": \"{account_id}\"\n",
    "                }},\n",
    "                \"ArnEquals\": {{\n",
    "                    \"aws:SourceArn\": \"arn:aws:bedrock:{region}:{account_id}:model-invocation-job/*\"\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5804c-5c1f-4571-8d17-2835f4b6a57f",
   "metadata": {},
   "source": [
    "### Create S3 access policy\n",
    "\n",
    "This JSON object defines the permissions of the role we want bedrock to assume to allow access to the S3 bucket that we created that will hold our prompts and allow certain bucket and object manipulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc2b82-d4cd-447d-96ab-bb2755de6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_POLICY_DOC = f\"\"\"{{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {{\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:GetBucketAcl\",\n",
    "                \"s3:GetBucketNotification\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutBucketNotification\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{bucket_name}\",\n",
    "                \"arn:aws:s3:::{bucket_name}/*\"\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc581e-6b38-4c74-a5f6-fd1d5b9c2e88",
   "metadata": {},
   "source": [
    "### Create IAM role and attach policies\n",
    "\n",
    "Let's now create the IAM role with the created trust policy and attach the s3 policy to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d17dc9-1a30-4fe3-a81d-13300a49fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = iam_client.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=ROLE_DOC,\n",
    "    Description=\"Role for Bedrock to access S3 for model invocation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a54f5-d9b9-4c81-973f-e95df69c0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_arn = response[\"Role\"][\"Arn\"]\n",
    "response = iam_client.create_policy(\n",
    "    PolicyName=s3_bedrock_ft_access_policy,\n",
    "    PolicyDocument=ACCESS_POLICY_DOC,\n",
    ")\n",
    "policy_arn = response[\"Policy\"][\"Arn\"]\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=policy_arn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c62964-9978-44df-9e90-8a6444666f9a",
   "metadata": {},
   "source": [
    "### Configure the model invocation job\n",
    "#### Create the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518f29c-3740-4fdb-a2b6-3d564983c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"batch-images\"\n",
    "input_key = \"input.jsonl\"\n",
    "output_path = \"validation/output/\"\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "data = []\n",
    "\n",
    "# Supported image file extensions\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "\n",
    "# Iterate over files in the folder\n",
    "for i, file_name in enumerate(os.listdir(folder_path)):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    _, extension = os.path.splitext(file_path)\n",
    "    if extension.lower() in image_extensions:\n",
    "        image = Image.open(file_path)\n",
    "        if image.height > 2048 or image.width > 2048:\n",
    "            resize_image(file_path)\n",
    "\n",
    "        model_input = {\n",
    "            \"inputImage\": image_to_base64(file_path),\n",
    "            \"embeddingConfig\": {\n",
    "                \"outputEmbeddingLength\": outputEmbeddingLength \n",
    "            }\n",
    "        }\n",
    "\n",
    "        data.append({'recordId': file_name, 'modelInput': model_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3f375-0a34-4417-b533-eb4d63dba9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input data items are:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096387e-b06e-483c-b18c-e452eb77402f",
   "metadata": {},
   "source": [
    "#### Process data and output to new lines\n",
    "The model invocation job requires the input data to be in jsonl format and located Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882dc51-7952-4e02-97b6-ad071b6e40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"\"\n",
    "for row in data:\n",
    "    output_data += json.dumps(row) + \"\\n\"\n",
    "\n",
    "s3_client.put_object(Body=output_data, Bucket=bucket_name, Key=input_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d690fd7-b8a0-4879-bef2-7fd8ab68189d",
   "metadata": {},
   "source": [
    "#### Define data configuration and launch job\n",
    "As the input data is prepared and uploaded to Amazon S3 we can go ahead and launch the invocation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b10e362-9949-4ec8-80cb-65ddfe8618cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3InputFormat\": \"JSONL\",\n",
    "        \"s3Uri\": \"{}/{}\".format(s3_bucket_path, input_key)\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": \"{}/{}\".format(s3_bucket_path, output_path)\n",
    "    }\n",
    "})\n",
    "date_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "response = bedrock_client.create_model_invocation_job(\n",
    "    roleArn=role_arn,\n",
    "    modelId=embeddings_model_arn_2,\n",
    "    jobName=f\"my-batch-job-test-{date_time}\",\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")\n",
    "\n",
    "jobArn = response.get('jobArn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aca610-386a-4a10-ac81-239753d5b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "status = bedrock_client.get_model_invocation_job(jobIdentifier=jobArn)['status']\n",
    "while status not in [\"Completed\", \"Failed\", \"Stopping\", \"Stopped\"]:\n",
    "    status = bedrock_client.get_model_invocation_job(jobIdentifier=jobArn)['status']\n",
    "    print(status)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a81d4-3997-4940-b93a-dc8dcb5946cb",
   "metadata": {},
   "source": [
    "#### Retrieve the embeddings \n",
    "Once the batch job is complete we can go ahead and download the output file and extract the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34480ee1-75cd-4659-847e-4e0dba9c959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = jobArn.split(\"/\")[-1]\n",
    "images_file = \"input.jsonl.out\"\n",
    "s3_client.download_file(bucket_name, \"{}{}/{}\".format(output_path, job_id, images_file), images_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664317a-bcff-4454-bddb-17f1fef5ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_embedding_list = []\n",
    "with open('input.jsonl.out', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        record_id = data['recordId']\n",
    "        embedding = data['modelOutput']['embedding']\n",
    "        record_embedding = {'recordId': record_id, 'embedding': embedding}\n",
    "        record_embedding_list.append(record_embedding)\n",
    "print(\"The embedding list contains {} records.\".format(len(record_embedding_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b43883-0d3e-4a2d-ae9b-bdddcb755279",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "In this section we will delete any resource which may incur in unnecessary costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9df69-33e0-47cc-96ed-6efea7ef52ed",
   "metadata": {},
   "source": [
    "#### Delete the Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733dce5c-10e0-4ee7-a036-b61d434b4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all objects in the bucket\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            s3_client.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "        print(f\"All objects in {bucket_name} have been deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting objects from {bucket_name}: {e}\")\n",
    "\n",
    "# Delete the bucket\n",
    "try:\n",
    "    response = s3_client.delete_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket {bucket_name} has been deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting bucket {bucket_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9e3c6-da65-4b0d-a55f-766b7a6f14cf",
   "metadata": {},
   "source": [
    "#### Delete the Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385de4d-7c3d-47bd-abf6-bad13446d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
