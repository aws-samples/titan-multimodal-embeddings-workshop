{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fda5b7-4a45-464c-85f1-e7472f47df8d",
   "metadata": {},
   "source": [
    "# Titan Multimodal Embeddings & Opensearch\n",
    "## Retail Example -- Dataset Preparation\n",
    "\n",
    "In this notebook we are going to download and prepare the dataset we are going to be using for a search example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e369a10-8e61-4221-9369-2793d19579dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579b780-b27b-448a-a792-8bfb3245dde9",
   "metadata": {},
   "source": [
    "### Download the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2137ea9-ab59-46e7-9319-bc8d1d838083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-04 12:51:52--  https://amazon-berkeley-objects.s3.us-east-1.amazonaws.com/archives/abo-listings.tar\n",
      "Resolving amazon-berkeley-objects.s3.us-east-1.amazonaws.com (amazon-berkeley-objects.s3.us-east-1.amazonaws.com)... 16.182.65.82, 52.217.114.242, 52.217.164.146, ...\n",
      "Connecting to amazon-berkeley-objects.s3.us-east-1.amazonaws.com (amazon-berkeley-objects.s3.us-east-1.amazonaws.com)|16.182.65.82|:443... connected.\n",
      "WARNING: cannot verify amazon-berkeley-objects.s3.us-east-1.amazonaws.com's certificate, issued by ‘CN=Amazon RSA 2048 M01,O=Amazon,C=US’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 87480320 (83M) [application/x-tar]\n",
      "Saving to: ‘abo-listings.tar’\n",
      "\n",
      "abo-listings.tar    100%[===================>]  83.43M  40.6MB/s    in 2.1s    \n",
      "\n",
      "2024-04-04 12:51:54 (40.6 MB/s) - ‘abo-listings.tar’ saved [87480320/87480320]\n",
      "\n",
      "--2024-04-04 12:51:54--  https://amazon-berkeley-objects.s3.us-east-1.amazonaws.com/images/metadata/images.csv.gz\n",
      "Resolving amazon-berkeley-objects.s3.us-east-1.amazonaws.com (amazon-berkeley-objects.s3.us-east-1.amazonaws.com)... 52.217.231.34, 54.231.140.42, 52.216.89.240, ...\n",
      "Connecting to amazon-berkeley-objects.s3.us-east-1.amazonaws.com (amazon-berkeley-objects.s3.us-east-1.amazonaws.com)|52.217.231.34|:443... connected.\n",
      "WARNING: cannot verify amazon-berkeley-objects.s3.us-east-1.amazonaws.com's certificate, issued by ‘CN=Amazon RSA 2048 M01,O=Amazon,C=US’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6430535 (6.1M) [text/csv]\n",
      "Saving to: ‘images.csv.gz’\n",
      "\n",
      "images.csv.gz       100%[===================>]   6.13M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-04-04 12:51:54 (89.7 MB/s) - ‘images.csv.gz’ saved [6430535/6430535]\n",
      "\n",
      "LICENSE-CC-BY-4.0.txt\n",
      "listings/\n",
      "listings/README.md\n",
      "listings/metadata/\n",
      "listings/metadata/listings_7.json.gz\n",
      "listings/metadata/listings_4.json.gz\n",
      "listings/metadata/listings_2.json.gz\n",
      "listings/metadata/listings_c.json.gz\n",
      "listings/metadata/listings_6.json.gz\n",
      "listings/metadata/listings_0.json.gz\n",
      "listings/metadata/listings_9.json.gz\n",
      "listings/metadata/listings_e.json.gz\n",
      "listings/metadata/listings_1.json.gz\n",
      "listings/metadata/listings_5.json.gz\n",
      "listings/metadata/listings_3.json.gz\n",
      "listings/metadata/listings_d.json.gz\n",
      "listings/metadata/listings_f.json.gz\n",
      "listings/metadata/listings_8.json.gz\n",
      "listings/metadata/listings_a.json.gz\n",
      "listings/metadata/listings_b.json.gz\n"
     ]
    }
   ],
   "source": [
    "!wget https://amazon-berkeley-objects.s3.us-east-1.amazonaws.com/archives/abo-listings.tar --no-check-certificate\n",
    "!wget https://amazon-berkeley-objects.s3.us-east-1.amazonaws.com/images/metadata/images.csv.gz --no-check-certificate\n",
    "!mkdir items-metadata\n",
    "!gzip -d images.csv.gz\n",
    "!tar xvf abo-listings.tar -C items-metadata\n",
    "!rm -f abo-listings.tar\n",
    "!gzip -d items-metadata/listings/metadata/listings_0.json.gz\n",
    "!cp items-metadata/listings/metadata/listings_0.json listings_0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1c872-2f3c-4c7c-9dc3-d586556c21fd",
   "metadata": {},
   "source": [
    "### Load the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3321d2f-eae0-4a6d-a27e-78560769bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items metadata contains 9232 records\n"
     ]
    }
   ],
   "source": [
    "items_file = \"listings_0.json\"\n",
    "items_metadata = []\n",
    "\n",
    "with open(items_file, 'r') as json_file:\n",
    "    items_metadata = list(map(json.loads, list(json_file)))\n",
    "\n",
    "print(\"items metadata contains {} records\".format(len(items_metadata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d139477-9f57-470c-a6fe-0e7502210fbf",
   "metadata": {},
   "source": [
    "### Create the curated dataset\n",
    "We are only going to use products with english descriptions and color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7130a48-15fe-4bb2-ba36-1ac5ca964537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 741 items in with data in English\n"
     ]
    }
   ],
   "source": [
    "en_US_items = list(filter(\n",
    "    lambda l: any(\n",
    "        c['language_tag'] == \"en_US\" for c in l.get('color', [])\n",
    "    ) and any(\n",
    "        i['language_tag'] == \"en_US\" for i in l.get('item_name', [])\n",
    "    ) and any(\n",
    "        bp['language_tag'] == \"en_US\" for bp in l.get('bullet_point', [])\n",
    "    ),\n",
    "    items_metadata\n",
    "))\n",
    "print(\"There are {} items in with data in English\".format(len(en_US_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5f2f2-7d6d-47b3-bf38-3dd4cc462d34",
   "metadata": {},
   "source": [
    "Now we have our items, let's create a list containing the item data and image location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "080e7389-4bab-49e1-acfb-e720c5ca75c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing items with duplicated images:\n",
      "Curated dataset contains 711 records.\n"
     ]
    }
   ],
   "source": [
    "# Load image information from CSV file into a dictionary\n",
    "image_info = {}\n",
    "\n",
    "with open('images.csv', 'r') as csvfile:\n",
    "    csvreader = csv.DictReader(csvfile)\n",
    "    for row in csvreader:\n",
    "        image_id = row['image_id']\n",
    "        image_info[image_id] = {\n",
    "            'height': int(row['height']),\n",
    "            'width': int(row['width']),\n",
    "            'path': row['path']\n",
    "        }\n",
    "        \n",
    "curated_dataset = []\n",
    "\n",
    "for item in en_US_items:\n",
    "    item_id = item.get('item_id', '')\n",
    "    item_name = item.get('item_name', [{'value': ''}])[0]['value']\n",
    "    color = item.get('color', [{'value': ''}])[0]['value']\n",
    "    brand = item.get('brand', [{'value': ''}])[0]['value']\n",
    "    bullet_points = [bp['value'] for bp in item.get('bullet_point', [])]\n",
    "    description = ' '.join(bullet_points)\n",
    "\n",
    "    image_id = item.get('main_image_id', '')\n",
    "    image_info_item = image_info.get(image_id, {'height': 0, 'width': 0, 'path': ''})\n",
    "    image_path = image_info_item['path']\n",
    "\n",
    "    curated_dataset.append({\n",
    "        'item_id': item_id,\n",
    "        'item_name': item_name,\n",
    "        'color': color,\n",
    "        'brand': brand,\n",
    "        'description': description,\n",
    "        'image_path': image_path\n",
    "    })\n",
    "\n",
    "image_paths = [item['image_path'] for item in curated_dataset]\n",
    "duplicate_image_paths = [path for path in image_paths if image_paths.count(path) > 1]\n",
    "\n",
    "if duplicate_image_paths:\n",
    "    print(\"Removing items with duplicated images:\")\n",
    "    for path in set(duplicate_image_paths):\n",
    "        duplicated_items = [item for item in curated_dataset if item['image_path'] == path]\n",
    "        #print(f\"Image path: {path}, Removed items: {duplicated_items}\")\n",
    "        curated_dataset = [item for item in curated_dataset if item['image_path'] != path]\n",
    "else:\n",
    "    print(\"No items with duplicated images.\")\n",
    "\n",
    "print(\"Curated dataset contains {} records.\".format(len(curated_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2049d-df10-43d6-a5d9-dd5c4506f049",
   "metadata": {},
   "source": [
    "### Download the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a06c0-bdcd-4e39-86fa-e6ca8c1dec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Folder to store downloaded images\n",
    "download_folder = 'images'\n",
    "\n",
    "# Create the download folder if it doesn't exist\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder)\n",
    "\n",
    "# Download images using wget and remove items with download errors\n",
    "curated_dataset_updated = []\n",
    "\n",
    "for item in curated_dataset:\n",
    "    image_path = item['image_path']\n",
    "    image_url = f'https://amazon-berkeley-objects.s3.us-east-1.amazonaws.com/images/original/{image_path}'\n",
    "    image_filename = os.path.join(download_folder, os.path.basename(image_path))\n",
    "\n",
    "    # Use subprocess to run wget command\n",
    "    result = subprocess.run(['wget', image_url,'--no-check-certificate', '-O', image_filename])\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        # Image downloaded successfully, update the image_path in curated_dataset_updated\n",
    "        item['image_path'] = os.path.basename(image_path)\n",
    "        curated_dataset_updated.append(item)\n",
    "        \n",
    "print(\"Images downloaded successfully.\")\n",
    "\n",
    "# Update curated_dataset with only the items that were successfully downloaded\n",
    "curated_dataset = curated_dataset_updated\n",
    "print(f\"Number of items after removing failed downloads: {len(curated_dataset)}\")\n",
    "\n",
    "print(\"Curated dataset with updated image paths saved to 'curated_dataset_updated.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ce5c9b3-7051-4f47-9500-4b0564939baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a JSON file\n",
    "with open('curated_dataset.json', 'w') as jsonfile:\n",
    "    json.dump(curated_dataset, jsonfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6af1cd7c-1b46-4b2e-8723-35e38188f674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in images: 710\n",
      "Number of items in curated_dataset is: 710\n"
     ]
    }
   ],
   "source": [
    "num_images = len([f for f in os.listdir(download_folder) if os.path.isfile(os.path.join(download_folder, f))])\n",
    "print(f\"Number of images in {download_folder}: {num_images}\")\n",
    "print(\"Number of items in curated_dataset is: {}\".format(len(curated_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53c71a-c475-421f-9ced-e0c6066e16ec",
   "metadata": {},
   "source": [
    "### Clean files not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aabdab71-ca15-44e5-a1cc-e51253e21bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f images.csv\n",
    "!rm -r items-metadata/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
